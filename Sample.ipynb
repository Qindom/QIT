{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qit.Qclient import Qclient\n",
    "from qit.utils import data_utils\n",
    "from qit.utils import preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"your_qit_access_user_name\"\n",
    "secrect_key = 'your_qit_access_secrect_key'\n",
    "client = Qclient(username,secrect_key)\n",
    "datasets = ['bank', 'biomechanical']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can use your own data, data provided by Qindom is just for demonstration\n",
    "data_df = pd.DataFrame(columns = ['train_X' , 'valid_X', 'train_y' , 'valid_y','train_X_y'], index=datasets)\n",
    "for dataset in datasets:\n",
    "    print(\"start data preparation for dataset: \" + dataset)\n",
    "    \n",
    "    train_valid_X, train_valid_y = data_utils.get_data(dataset)\n",
    "    # Split the data for train and validation\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(train_valid_X, train_valid_y, train_size=.7)\n",
    "    train_X_y = np.concatenate((train_X, train_y.reshape(train_y.size,1)), axis=1)\n",
    "    \n",
    "    data_df['train_X'][dataset] = train_X\n",
    "    data_df['train_y'][dataset] = train_y\n",
    "    data_df['valid_X'][dataset] = valid_X\n",
    "    data_df['valid_y'][dataset] = valid_y\n",
    "    data_df['train_X_y'][dataset] = train_X_y\n",
    "    \n",
    "    np.savetxt(\"c:\\\\users\\\\fores\\\\Demo\\\\data\\\\\" + dataset + \"_train_X_y.csv\", train_X_y, delimiter=',') #change to your own directory\n",
    "    np.savetxt(\"c:\\\\users\\\\fores\\\\Demo\\\\data\\\\\" + dataset + \"_valid_X.csv\", valid_X, delimiter=',') #change to your own directory\n",
    "    for data in ['train_X_y','valid_X']:\n",
    "        #change to your own directory\n",
    "        cmdln = \"C:\\\\Amazon\\\\AWSCLI\\\\bin\\\\aws.cmd s3 cp C:\\\\Users\\\\fores\\\\Demo\\\\data\\\\\" + dataset + \"_\" + data + \".csv s3://qitdata/\" + username + \"/\" + dataset + \"_\" + data + \".csv\"\n",
    "        p = subprocess.Popen(cmdln, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "        for line in p.stdout.readlines():\n",
    "            print(line)\n",
    "            \n",
    "    print(\"finished data preparation for dataset:\" + dataset)\n",
    "    \n",
    "print(\"Finished all data preparation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_df = pd.DataFrame(columns = ['Qit'], index=datasets)\n",
    "prd_df = pd.DataFrame(columns = ['Qit'], index=datasets)\n",
    "for dataset in datasets:\n",
    "    \n",
    "    print(\"start to train model for: \" + dataset)\n",
    "\n",
    "    train_X = data_df['train_X'][dataset]\n",
    "    train_y = data_df['train_y'][dataset]\n",
    "    train_X_y = data_df['train_X_y'][dataset]\n",
    "    \n",
    "    model_id = client.train_model_s3(training_data=username +'/' + dataset + '_train_X_y.csv', clf_type='QBOOST_IT')\n",
    "    clf_df['Qit'][dataset] = model_id['body']\n",
    "    print(\"model id:\" + str(clf_df['Qit'][dataset]))\n",
    "    \n",
    "    print(\"start predict for dataset: \" + dataset)\n",
    "    \n",
    "    valid_X = data_df['valid_X'][dataset]\n",
    "    valid_y = data_df['valid_y'][dataset]\n",
    "    \n",
    "#Get prediction result from Qit    \n",
    "    prediction_result = client.predict_sync(test_data=username + '/' + dataset + '_valid_X.csv', modelId=clf_df['Qit'][dataset]) \n",
    "    while 'Models are not ready yet' in prediction_result:\n",
    "        print(prediction_result + \", will try after a minute\")\n",
    "        time.sleep(60)\n",
    "        prediction_result = client.predict_sync(test_data=username + '/' + dataset + '_valid_X.csv', modelId=clf_df['Qit'][dataset])\n",
    "    prd_df['Qit'][dataset] = np.array(json.loads(prediction_result)['y_test_pred'])\n",
    "    \n",
    "#Calculate accuracy   \n",
    "    qit_accuracy = metrics.accuracy_score(valid_y, prd_df['Qit'][dataset])\n",
    "  \n",
    "    print(\"finished predict for dataset: \" + dataset + \" with accuracy \" + qit_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
